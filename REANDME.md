# Neural Network (NumPy Only)

> 为了搞懂深度学习到底是怎么生效的，我用 NumPy 来探索这个微型神经网络。


## 为什么做这个项目？(Why?)

在大一接触深度学习时，我发现调用 `torch.nn.Linear` 或 `model.fit()` 非常简单，但我总觉得心里没底：
* 梯度到底是怎么传回去的？
* 机器是怎么把“7”看成“1”的？
* 反向传播的链式法则在代码里长什么样？

为了回答这些问题，我决定**不使用任何深度学习框架 (No PyTorch/TensorFlow)**，只用基础的线性代数库 (NumPy)，亲手推导并实现一个能识别手写数字的神经网络。

## 我实现了什么？(Features)

这是一个**探索性的实验代码**：

1.  **手推反向传播 (Backpropagation)**
    * 没有用自动求导机制，而是手动推导了全连接层 ($Y=XW+b$) 的梯度公式，并用代码实现了链式法则。
    * *感受：写完这部分才真正理解了微积分在 AI 里的意义。*

2.  **纯矩阵运算 (Matrix Operations)**
    * 前向传播就是矩阵乘法。
    * 实现了 **He Initialization** (权重初始化) 和 **ReLU** 激活函数。

3.  **从训练到推理的闭环**
    * 实现了 `train.py` (训练) 和 `predict.py` (用自己的手绘图测试)。
    * 包含模型权重的保存与加载功能。

### 1. 成功时刻
* 在 MNIST 测试集上，模型达到了 **97.15%** 的准确率。
* Loss 曲线平稳下降，证明手动推导的梯度公式是正确的。

### 2. 探索中的“Bug” (My Learnings)
在测试我自己手写的数字时，我发现了全连接层(MLP)的**局限性**：
如果我把数字写偏了，或者太细，模型就会认错（比如把 7 认成 1）。如果我写的“7”太斜，模型会因为它和“8”的像素重合度高而误判。这让我直观地理解了为什么我们需要 **CNN (卷积神经网络)** —— 因为 MLP 缺乏对形状和拓扑结构的理解。

## Quick Start

非常简单，只需要安装 `numpy` 和 `pillow` (用于处理图片)。

```bash
# 1. 安装依赖
pip install numpy pillow tensorflow
# (注: tensorflow 仅用于下载 MNIST 数据集，不参与训练)

# 2. 开始训练 (见证 Loss 下降)
python train.py

# 3. 用你自己的图测试 (记得画粗一点!)
python predict.py